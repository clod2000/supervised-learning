{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shbM3zJAuPI9"
   },
   "source": [
    "In this lab you will do the following steps in order:\n",
    "\n",
    "1. Explore and Load dataset.\n",
    "2. Train [faster RCNN](https://towardsdatascience.com/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46) for uno cards object detection.\n",
    "3. Test the network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F0vCe0kzcJ9"
   },
   "source": [
    "Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bun1lQdwoqy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "DEVICE = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWU1vPVJVcXS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEJy1NWGK0iA"
   },
   "source": [
    "1. Explore and Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPv85FEZK4W1"
   },
   "outputs": [],
   "source": [
    "#!gdown \"13IjqxWIVsP2bgPZP6sE9_VYGXMeu1MWI\"\n",
    "#!unzip -q -n \"/lab08/Uno Cards.v2-raw.voc.zip\" -d /lab08/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbO_Ir1VMTOv"
   },
   "source": [
    "Setup of the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iz_2sPIMNgQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "BATCH_SIZE = 8 # increase / decrease according to GPU memeory\n",
    "RESIZE_TO = 416 # resize the image for training and transforms\n",
    "NUM_EPOCHS = 1 # number of epochs to train for\n",
    "NUM_WORKERS = 4\n",
    "# training images and XML files directory\n",
    "TRAIN_DIR = 'data/train'\n",
    "# validation images and XML files directory\n",
    "VALID_DIR = 'data/valid'\n",
    "TEST_DIR = 'data/test'\n",
    "# classes: 0 index is reserved for background\n",
    "CLASSES = ['__background__','0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14']\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "# whether to visualize images after crearing the data loaders\n",
    "VISUALIZE_TRANSFORMED_IMAGES = True\n",
    "# location to save model and plots\n",
    "OUT_DIR = 'outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9-E4csmMS6q"
   },
   "outputs": [],
   "source": [
    "from IPython.lib.display import exists\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "plt.style.use('ggplot')\n",
    "# this class keeps track of the training and validation loss values...\n",
    "# ... and helps to get the average for each epoch as well\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's\n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "\n",
    "    def __call__(\n",
    "        self, current_valid_loss,\n",
    "        epoch, model, optimizer\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            os.makedirs(\"outputs\",exist_ok=True)\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/best_model.pth')\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    To handle the data loading as different images may have different number\n",
    "    of objects and to handle varying size tensors as well.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "# define the training tranformations to avoid overfitting of the network\n",
    "# rotations motionblur, medianblur, and blur are added to the training...\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(p=0.5),  # Correctly specifying the probability parameter\n",
    "        A.RandomRotate90(p=0.5),  # Correctly specifying the probability parameter\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        A.Blur(blur_limit=3, p=0.1),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc',\n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "\n",
    "# define the validation transforms\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc',\n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "def show_transformed_image(train_loader):\n",
    "    \"\"\"\n",
    "    This function visualizes transformed images from the `train_loader`.\n",
    "\n",
    "    It helps to verify if the transformations applied to the images and their\n",
    "    corresponding labels are correct. The function only executes if the flag\n",
    "    `VISUALIZE_TRANSFORMED_IMAGES` is set to `True` in the configuration file (`config.py`).\n",
    "\n",
    "    Args:\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader containing\n",
    "                                                  transformed training images and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(train_loader) > 0:\n",
    "        # Iterate over the data loader to get one batch of images and labels\n",
    "        for i in range(1):\n",
    "            images, targets = next(iter(train_loader))\n",
    "\n",
    "            # Move images to the specified device (CPU or GPU)\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "\n",
    "            # Move labels to the specified device and convert non-string elements to tensors on that device\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items() if not isinstance(v, str)} for t in targets]\n",
    "\n",
    "            # Extract bounding boxes and labels from the first target in the batch\n",
    "            boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "            labels = targets[i]['labels'].cpu().numpy().astype(np.int32)\n",
    "\n",
    "            # Get the first image from the batch and convert it to a NumPy array for visualization\n",
    "            sample = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "            # Draw bounding boxes and labels on the image\n",
    "            for box_num, box in enumerate(boxes):\n",
    "                cv2.rectangle(sample,\n",
    "                             (box[0], box[1]),  # Top-left corner coordinates\n",
    "                             (box[2], box[3]),  # Bottom-right corner coordinates\n",
    "                             (0, 0, 255), 2)    # Red color, thickness 2\n",
    "                label_text = CLASSES[labels[box_num]]  # Get label text from label index\n",
    "                cv2.putText(sample, label_text,\n",
    "                            (box[0], box[1] - 10),  # Position above top-left corner\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)  # Font style, size, color, thickness\n",
    "\n",
    "            # Display the image using matplotlib\n",
    "            plt.imshow(sample)\n",
    "            plt.axis(\"off\")  # Hide axes for cleaner visualization\n",
    "            plt.show()\n",
    "\n",
    "def save_model(epoch, model, optimizer):\n",
    "    \"\"\"\n",
    "    Function to save the trained model till current epoch, or whenver called\n",
    "    \"\"\"\n",
    "    os.makedirs(\"outputs\",exist_ok=True)\n",
    "    torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/last_model.pth')\n",
    "def save_loss_plot(OUT_DIR, train_loss, val_loss):\n",
    "    figure_1, train_ax = plt.subplots()\n",
    "    figure_2, valid_ax = plt.subplots()\n",
    "    train_ax.plot(train_loss, color='tab:blue')\n",
    "    train_ax.set_xlabel('iterations')\n",
    "    train_ax.set_ylabel('train loss')\n",
    "    valid_ax.plot(val_loss, color='tab:red')\n",
    "    valid_ax.set_xlabel('iterations')\n",
    "    valid_ax.set_ylabel('validation loss')\n",
    "    figure_1.savefig(f\"{OUT_DIR}/train_loss.png\")\n",
    "    figure_2.savefig(f\"{OUT_DIR}/valid_loss.png\")\n",
    "    print('SAVING PLOTS COMPLETE...')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSe-IrkVOD2g"
   },
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpqAT4mpOARD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob as glob\n",
    "from xml.etree import ElementTree as et\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# the dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    # i want a fix size for images, and number of classes (one of them is background) and so on\n",
    "    def __init__(self, dir_path, width, height, classes, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.dir_path = dir_path\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.classes = classes\n",
    "\n",
    "        # get all the image paths in sorted order\n",
    "        self.image_paths = glob.glob(f\"{self.dir_path}/*.jpg\")\n",
    "        self.all_images = [image_path.split(os.path.sep)[-1] for image_path in self.image_paths]\n",
    "        self.all_images = sorted(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # capture the image name and the full image path\n",
    "        image_name = self.all_images[idx]\n",
    "        image_path = os.path.join(self.dir_path, image_name)\n",
    "        # read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        # convert BGR to RGB color format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "\n",
    "        # capture the corresponding XML file for getting the annotations\n",
    "        annot_filename = image_name[:-4] + '.xml'\n",
    "        annot_file_path = os.path.join(self.dir_path, annot_filename)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # get the height and width of the image\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "\n",
    "        # box coordinates for xml files are extracted and corrected for image size given\n",
    "        for member in root.findall('object'):\n",
    "            # map the current object name to `classes` list to get...\n",
    "            # ... the label index and append to `labels` list\n",
    "            labels.append(self.classes.index(member.find('name').text))\n",
    "\n",
    "            # xmin = left corner x-coordinates\n",
    "            xmin = int(member.find('bndbox').find('xmin').text)\n",
    "            # xmax = right corner x-coordinates\n",
    "            xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            # ymin = left corner y-coordinates\n",
    "            ymin = int(member.find('bndbox').find('ymin').text)\n",
    "            # ymax = right corner y-coordinates\n",
    "            ymax = int(member.find('bndbox').find('ymax').text)\n",
    "\n",
    "            # resize the bounding boxes according to the desired `width`, `height`\n",
    "            xmin_final = (xmin/image_width)*self.width\n",
    "            xmax_final = (xmax/image_width)*self.width\n",
    "            ymin_final = (ymin/image_height)*self.height\n",
    "            yamx_final = (ymax/image_height)*self.height\n",
    "            boxes.append([xmin_final, ymin_final, xmax_final, yamx_final])\n",
    "\n",
    "        # bounding box to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # no crowd instances\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        # labels to tensor\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # prepare the final `target` dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"name\"]=image_name\n",
    "        # apply the image transforms\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = image_resized,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "\n",
    "        return image_resized, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "# prepare the final datasets and data loaders\n",
    "def create_train_dataset():\n",
    "    train_dataset = CustomDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_train_transform())\n",
    "    return train_dataset\n",
    "def create_valid_dataset():\n",
    "    valid_dataset = CustomDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\n",
    "    return valid_dataset\n",
    "def create_test_dataset():\n",
    "    valid_dataset = CustomDataset(TEST_DIR, RESIZE_TO, RESIZE_TO, CLASSES, get_valid_transform())\n",
    "    return valid_dataset\n",
    "def create_train_loader(train_dataset, num_workers=0):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_loader\n",
    "def create_valid_loader(valid_dataset, num_workers=0, batch_size=BATCH_SIZE):\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return valid_loader\n",
    "\n",
    "dataset = CustomDataset(\n",
    "    TRAIN_DIR, RESIZE_TO, RESIZE_TO, CLASSES\n",
    ")\n",
    "print(f\"Number of training images: {len(dataset)}\")\n",
    "\n",
    "# function to visualize a single sample\n",
    "def visualize_sample(image, target):\n",
    "    image=image*255\n",
    "    for box_num in range(len(target['boxes'])):\n",
    "        box = target['boxes'][box_num]\n",
    "        label = CLASSES[target['labels'][box_num]]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])), (int(box[2]), int(box[3])),\n",
    "            (0, 255, 0), 2\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image, label, (int(box[0]), int(box[1]-5)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2\n",
    "        )\n",
    "    image=image.astype(np.uint8)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "NUM_SAMPLES_TO_VISUALIZE = 5\n",
    "for i in range(NUM_SAMPLES_TO_VISUALIZE):\n",
    "    image, target = dataset[i]\n",
    "    visualize_sample(image, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWs3idSERZrQ"
   },
   "source": [
    "Model setup. [Faster-rcnn](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html?highlight=fasterrcnn_resnet50_fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxNxUCl2RYFF"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "def create_model(num_classes):\n",
    "\n",
    "    # load Faster RCNN pre-trained model\n",
    "    # model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"FasterRCNN_ResNet50_FPN_Weights.COCO_V1\")\n",
    "\n",
    "    # get the number of input features\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvnweGDQQXmF"
   },
   "source": [
    "2. Train and validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxniAtWqQbra"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train(train_data_loader, model):\n",
    "    \"\"\"\n",
    "    This function trains the model on the provided training data loader.\n",
    "\n",
    "    Args:\n",
    "        train_data_loader (torch.utils.data.DataLoader): The data loader containing training images and targets.\n",
    "        model (torch.nn.Module): The deep learning model to be trained.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the training loss values for each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Training')\n",
    "\n",
    "    global train_itr  # Assuming this tracks the current training iteration (potentially for logging)\n",
    "    global train_loss_list  # Assuming this stores training loss values (potentially for monitoring)\n",
    "\n",
    "    # Initialize tqdm progress bar for training loop visualization\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        # Clear optimizer gradients for each iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get images and targets from the current batch\n",
    "        images, targets = data\n",
    "\n",
    "        # Move images to the specified device (CPU or GPU)\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "\n",
    "        # Move targets to the specified device and convert non-string elements to tensors on that device (gives error to move a string to GPU)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items() if not isinstance(v, str)} for t in targets]\n",
    "\n",
    "        # Forward pass through the model to get loss dictionary\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # Calculate the total loss by summing losses from the loss dictionary\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Extract the loss value as a scalar\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        # Append the loss value to the training loss list\n",
    "        train_loss_list.append(loss_value)\n",
    "\n",
    "        # Send the loss value to a visualization tool (e.g., TensorBoard) if applicable\n",
    "        train_loss_hist.send(loss_value)  # Assuming train_loss_hist is a visualization tool\n",
    "\n",
    "        # Perform backward pass to calculate gradients\n",
    "        losses.backward()\n",
    "\n",
    "        # Update model weights using optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update training iteration counter (potentially for logging)\n",
    "        train_itr += 1\n",
    "\n",
    "        # Update progress bar with current loss value\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "\n",
    "def validate(valid_data_loader, model):\n",
    "    \"\"\"\n",
    "    This function validates the model's performance on the provided validation data loader.\n",
    "\n",
    "    Args:\n",
    "        valid_data_loader (torch.utils.data.DataLoader): The data loader containing validation images and targets.\n",
    "        model (torch.nn.Module): The deep learning model to be validated.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the validation loss values for each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Validating')\n",
    "\n",
    "    global val_itr  # Assuming this tracks the current validation iteration (potentially for logging)\n",
    "    global val_loss_list  # Assuming this stores validation loss values (potentially for monitoring)\n",
    "\n",
    "    # Initialize tqdm progress bar for validation loop visualization\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        # Get images and targets from the current batch\n",
    "        images, targets = data\n",
    "\n",
    "        # Move images to the specified device (CPU or GPU)\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "\n",
    "        # Move targets to the specified device and convert non-string elements to tensors on that device\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items() if not isinstance(v, str)} for t in targets]\n",
    "\n",
    "        # Disable gradient calculation for validation (saves memory and computation)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through the model to get loss dictionary\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            # Calculate the total loss by summing losses from the loss dictionary\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Extract the loss value as a scalar\n",
    "            loss_value = losses.item()\n",
    "\n",
    "            # Append the loss value to the validation loss list\n",
    "            val_loss_list.append(loss_value)\n",
    "\n",
    "            # Send the loss value to a visualization tool (e.g., TensorBoard) if applicable\n",
    "            val_loss_hist.send(loss_value)  # Assuming val_loss_hist is a visualization tool\n",
    "\n",
    "            # Update validation iteration counter (potentially for logging)\n",
    "            val_itr += 1\n",
    "\n",
    "        # Update progress bar with current loss value\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "\n",
    "    return val_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8ubKo4xQq6_"
   },
   "source": [
    "Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5Vbje8gQqID"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_dataset = create_train_dataset()\n",
    "    valid_dataset = create_valid_dataset()\n",
    "    test_dataset = create_test_dataset()\n",
    "    train_loader = create_train_loader(train_dataset, NUM_WORKERS)\n",
    "    valid_loader = create_valid_loader(valid_dataset, NUM_WORKERS)\n",
    "    test_loader = create_valid_loader(test_dataset, NUM_WORKERS, batch_size=1)\n",
    "    print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
    "    print(f\"Number of test samples: {len(test_dataset)}\\n\")\n",
    "    # initialize the model and move to the computation device\n",
    "    model = create_model(num_classes=NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "    # get the model parameters\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    # define the optimizer\n",
    "    optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "    # initialize the Averager class\n",
    "    train_loss_hist = Averager()\n",
    "    val_loss_hist = Averager()\n",
    "    train_itr = 1\n",
    "    val_itr = 1\n",
    "    # train and validation loss lists to store loss values of all...\n",
    "    # ... iterations till ena and plot graphs for all iterations\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    # name to save the trained model with\n",
    "    MODEL_NAME = 'model'\n",
    "    # whether to show transformed images from data loader or not\n",
    "    if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "        show_transformed_image(train_loader)\n",
    "    # initialize SaveBestModel class\n",
    "    save_best_model = SaveBestModel()\n",
    "    # start the training epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "        # reset the training and validation loss histories for the current epoch\n",
    "        train_loss_hist.reset()\n",
    "        val_loss_hist.reset()\n",
    "        # start timer and carry out training and validation\n",
    "        start = time.time()\n",
    "        train_loss = train(train_loader, model)\n",
    "        val_loss = validate(valid_loader, model)\n",
    "        print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")\n",
    "        print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")\n",
    "        end = time.time()\n",
    "        print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "        # save the best model till now if we have the least loss in the      # ... current epoch\n",
    "        save_best_model(\n",
    "            val_loss_hist.value, epoch, model, optimizer\n",
    "        )\n",
    "        # save the current epoch model\n",
    "        save_model(epoch, model, optimizer)\n",
    "        # save loss plot\n",
    "        save_loss_plot(OUT_DIR, train_loss, val_loss)\n",
    "\n",
    "        # sleep for 5 seconds after each epoch\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_0nbYTBQ8eo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import glob as glob\n",
    "import os\n",
    "import time\n",
    "from copy import copy\n",
    "\n",
    "# load the best model and trained weights\n",
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "checkpoint = torch.load('outputs/best_model.pth', map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE).eval()\n",
    "# define the detection threshold...\n",
    "# ... any detection having score below this will be discarded\n",
    "detection_threshold = 0.8\n",
    "# to count the total number of images iterated through\n",
    "frame_count = 0\n",
    "# to keep adding the FPS for each image\n",
    "total_fps = 0\n",
    "os.makedirs(\"inference_outputs/images\",exist_ok=True)\n",
    "# initialize tqdm progress bar\n",
    "prog_bar = tqdm(test_loader, total=len(test_loader))\n",
    "\n",
    "for i, data in enumerate(prog_bar):\n",
    "    image, target = data\n",
    "    image=image[0]\n",
    "    target=target[0]\n",
    "    orig_image = np.array(image).transpose(1,2,0)\n",
    "    image=image.to(DEVICE)\n",
    "\n",
    "    # get the image file name for saving output later on\n",
    "    image_name = target[\"name\"]\n",
    "    bbox=target[\"boxes\"]\n",
    "    # print(image_name,target)\n",
    "\n",
    "    # add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image.to(DEVICE))\n",
    "    end_time = time.time()\n",
    "    # get the current fps\n",
    "    fps = 1 / (end_time - start_time)\n",
    "    # add `fps` to `total_fps`\n",
    "    total_fps += fps\n",
    "    # increment frame count\n",
    "    frame_count += 1\n",
    "    # load all detection to CPU for further operations\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "    # carry further only if there are detected boxes\n",
    "    # Check if there are any detected bounding boxes\n",
    "    if len(outputs[0]['boxes']) != 0:\n",
    "        # Get boxes and scores from the output dictionary\n",
    "        boxes = outputs[0]['boxes'].data.numpy()\n",
    "        scores = outputs[0]['scores'].data.numpy()\n",
    "\n",
    "        # Filter boxes based on a detection threshold (confidence score)\n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        draw_boxes = boxes.copy()  # Copy boxes for drawing\n",
    "\n",
    "        # Get predicted class names based on class indices\n",
    "        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "\n",
    "        # Loop through detected bounding boxes and draw them on the image\n",
    "        for j, box in enumerate(draw_boxes):\n",
    "            class_name = pred_classes[j]\n",
    "\n",
    "            # Draw a green rectangle around the bounding box\n",
    "            cv2.rectangle(orig_image, (int(box[0]), int(box[1])),\n",
    "                          (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "\n",
    "            # Write the predicted class name above the bounding box\n",
    "            cv2.putText(orig_image, class_name, (int(box[0]), int(box[1] - 5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Display the image with bounding boxes and class names using matplotlib\n",
    "        plt.imshow(orig_image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Save the image with bounding boxes to a specified directory\n",
    "        plt.savefig(f\"inference_outputs/images/{image_name}.jpg\", bbox_inches=\"tight\",pad_inches=0)\n",
    "        plt.show()\n",
    "        # cv2.imwrite(f\"inference_outputs/images/{image_name}.jpg\", np.uint8(orig_image[:,:,::-1]*255))\n",
    "        # Create a new figure for potentially further plotting needs\n",
    "        plt.figure()\n",
    "    print(f\"Image {i+1} done...\")\n",
    "    print('-'*50)  # Print a separator for clarity\n",
    "\n",
    "print('TEST PREDICTIONS COMPLETE')\n",
    "\n",
    "# Close any open OpenCV windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calculate and print the average FPS\n",
    "avg_fps = total_fps / frame_count\n",
    "print(f\"Average FPS: {avg_fps:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VfgyfEavT6C"
   },
   "source": [
    "**ASSIGNMENT**\n",
    "\n",
    "Display some images in the test set with the highest and lowest Intersection over Union (IoU) scores. Additionally, attempt to provide insights into why the model may encounter difficulties in detecting certain images."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
